{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1IblebdqvbutvShl3h_8Y7x0djlCCn81Y","authorship_tag":"ABX9TyPTSQROHaF9satbI9v9nP6I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install tableauhyperapi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7IZQLT9W0qu","executionInfo":{"status":"ok","timestamp":1746553150314,"user_tz":420,"elapsed":2418,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"2561deec-39a8-4c9e-ce98-2192a12b8083"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tableauhyperapi in /usr/local/lib/python3.11/dist-packages (0.0.22106)\n","Requirement already satisfied: cffi!=1.14.3,<2,>=1.12.2 in /usr/local/lib/python3.11/dist-packages (from tableauhyperapi) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi!=1.14.3,<2,>=1.12.2->tableauhyperapi) (2.22)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJarseZ_WRYG","executionInfo":{"status":"ok","timestamp":1746555510880,"user_tz":420,"elapsed":515296,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"de386304-9854-487f-8efa-49ab7b47dafb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n","Perhaps you already have a cluster running?\n","Hosting the HTTP server on port 40231 instead\n","  warnings.warn(\n","INFO:distributed.scheduler:State start\n","INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:32883\n","INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:40231/status\n","INFO:distributed.scheduler:Registering Worker plugin shuffle\n","INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:40841'\n","INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:43515 name: 0\n","INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:43515\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:59282\n","INFO:distributed.scheduler:Receive client connection: Client-5219b103-2aa5-11f0-9e21-0242ac1c000c\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:59288\n"]},{"output_type":"stream","name":"stdout","text":["Skipping partition 0 (already processed)\n","Skipping partition 1 (already processed)\n","Skipping partition 2 (already processed)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"]},{"output_type":"stream","name":"stdout","text":["Skipped partition 3 due to HyperException: COPY-IN cancelled.\n","Context: 0x3df1553f\n","Skipping partition 4 (already processed)\n","Skipping partition 5 (already processed)\n","Skipping partition 6 (already processed)\n","Skipping partition 7 (already processed)\n","\n","✅ Finished. Inserted: 0, Skipped: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:distributed.core:Event loop was unresponsive in Scheduler for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"]}],"source":["from dask_cuda import LocalCUDACluster\n","from dask.distributed import Client\n","import dask_cudf\n","from pathlib import Path\n","from tableauhyperapi import (\n","    HyperProcess, Telemetry,\n","    Connection, CreateMode,\n","    SchemaName, TableName,\n","    NULLABLE, SqlType, TableDefinition,\n","    Inserter, HyperException\n",")\n","\n","# Cluster\n","cluster = LocalCUDACluster()\n","client  = Client(cluster)\n","\n","# Constants\n","INT_COLS = [\"pickup_location_id\", \"dropoff_location_id\"]\n","TEXT_COLS = [\"pickup_date\", \"pickup_time\", \"dropoff_date\", \"dropoff_time\"]\n","\n","START_PARTITION = 3\n","\n","def run_create_hyper_file_from_parquet(parquet_file_path: Path,\n","        table_definition: TableDefinition,\n","        hyper_database_path: Path,\n","        npartitions: int = None):\n","\n","    ddf = dask_cudf.read_parquet(str(parquet_file_path))\n","    if npartitions:\n","        ddf = ddf.repartition(npartitions=npartitions)\n","\n","    for c in INT_COLS:\n","        ddf[c] = ddf[c].astype('float64')\n","        mask   = ddf[c].isnull() | ((ddf[c] % 1) == 0)\n","        ddf    = ddf[mask]\n","        ddf[c]  = ddf[c].astype('int64')\n","\n","\n","    for c in TEXT_COLS:\n","        ddf[c] = ddf[c].astype('str')\n","\n","    ddf = ddf.persist()\n","\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper, \\\n","         Connection(endpoint=hyper.endpoint,\n","                    database=hyper_database_path,\n","                    create_mode=CreateMode.CREATE_AND_REPLACE if not hyper_database_path.exists() else CreateMode.NONE) as conn:\n","\n","        # conn.catalog.create_schema(schema=SchemaName(\"Extract\"))\n","        # conn.catalog.create_table(table_definition=table_definition)\n","        total_inserted = total_skipped = 0\n","\n","        for i, part in enumerate(ddf.to_delayed()):\n","            if i != START_PARTITION:\n","                print(f\"Skipping partition {i} (already processed)\")\n","                continue\n","\n","            try:\n","                gdf = part.compute()\n","                pdf = gdf.to_pandas()\n","                rows = list(pdf.itertuples(index=False, name=None))\n","                with Inserter(conn, table_definition) as inserter:\n","                    inserter.add_rows(rows)\n","                    inserter.execute()\n","                    total_inserted += len(rows)\n","                    print(f\"Inserted partition {i} ({len(rows)} rows)\")\n","            except HyperException as e:\n","                print(f\"Skipped partition {i} due to HyperException: {e}\")\n","                total_skipped += 1\n","\n","        print(f\"\\n✅ Finished. Inserted: {total_inserted}, Skipped: {total_skipped}\")\n","\n","\n","if __name__ == \"__main__\":\n","    pq_path    = Path(\"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"Extract\", \"combined_yellow\"),\n","        columns=[\n","            TableDefinition.Column(\"pickup_date\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"trip_time\",             SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"passenger_count\",       SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"pickup_location_id\",    SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\",   SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"fare_amount\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"extra\",                 SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"mta_tax\",               SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tip_amount\",            SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tolls_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"improvement_surcharge\", SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"total_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\",  SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"Airport_fee\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",    SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"RatecodeID\",            SqlType.text(),    NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(\n","        pq_path,\n","        table_definition,\n","        hyper_path,\n","        npartitions=8\n","    )\n"]}]}