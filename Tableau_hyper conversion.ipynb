{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1SA7TATFc8DFVNOSVOX25DcQ3e5RXJhkE","authorship_tag":"ABX9TyM3ShVu3VG2U73CGin5a1it"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hC9wmQjMGxPl","executionInfo":{"status":"ok","timestamp":1746550003452,"user_tz":420,"elapsed":2316,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"88549409-0cff-470a-916c-66d5bc11f767"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tableauhyperapi in /usr/local/lib/python3.11/dist-packages (0.0.22106)\n","Requirement already satisfied: cffi!=1.14.3,<2,>=1.12.2 in /usr/local/lib/python3.11/dist-packages (from tableauhyperapi) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi!=1.14.3,<2,>=1.12.2->tableauhyperapi) (2.22)\n"]}],"source":["!pip install tableauhyperapi"]},{"cell_type":"markdown","source":["#Green"],"metadata":{"id":"4saCHQzUjZkZ"}},{"cell_type":"code","source":["import pyarrow.parquet as pq\n","\n","# point this at whichever file you want to inspect\n","parquet_path = \"/content/drive/MyDrive/green_trip_data/green_tripdata_processed.parquet\"\n","\n","'''\n","Path(\"/content/drive/MyDrive/fhvhv_trip_data/combined_fhvhv.parquet\"),\n","Path(\"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\"),\n","Path(\"/content/drive/MyDrive/green_trip_data/green_tripdata_processed.parquet\"),\n","'''\n","# create a ParquetFile object\n","pq_file = pq.ParquetFile(parquet_path)\n","\n","# print the Arrow schema (field names, types, nullability)\n","print(pq_file.schema_arrow)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbTqexXXcEUJ","executionInfo":{"status":"ok","timestamp":1746532360345,"user_tz":420,"elapsed":32,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"64509e26-6d33-46d6-e8ba-4f35064fd5cb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["pickup_date: date32[day]\n","pickup_time: time64[us]\n","dropoff_date: date32[day]\n","dropoff_time: time64[us]\n","trip_time: double\n","pickup_datetime: timestamp[us]\n","dropoff_datetime: timestamp[us]\n","pickup_location_id: int64\n","dropoff_location_id: int64\n","passenger_count: double\n","trip_distance: double\n","fare_amount: double\n","extra: double\n","mta_tax: double\n","tip_amount: double\n","tolls_amount: double\n","ehail_fee: null\n","improvement_surcharge: double\n","total_amount: double\n","congestion_surcharge: double\n","cbd_congestion_fee: double\n","fare_per_mile: double\n","RatecodeID: string\n","__null_dask_index__: int64\n","-- schema metadata --\n","pandas: '{\"index_columns\": [\"__null_dask_index__\"], \"column_indexes\": [{\"' + 3259\n"]}]},{"cell_type":"code","source":["from datetime import date\n","from pathlib import Path\n","\n","from tableauhyperapi import HyperProcess, Telemetry, \\\n","    Connection, CreateMode, \\\n","    NULLABLE, NULLABLE, SqlType, TableDefinition, TableName, \\\n","    Inserter, \\\n","    escape_name, escape_string_literal, \\\n","    HyperException\n","\n","\n","def run_create_hyper_file_from_parquet(\n","        parquet_file_path: Path,\n","        table_definition: TableDefinition,\n","        hyper_database_path: Path):\n","    \"\"\"\n","    An example demonstrating how to load rows from an Apache Parquet file (`parquet_file_path`)\n","    into a new Hyper file (`hyper_database_path`) using the COPY command. Currently the\n","    table definition of the data to copy needs to be known and explicitly specified.\n","\n","    Reading Parquet data is analogous to reading CSV data. For more details, see:\n","    https://tableau.github.io/hyper-db/docs/guides/hyper_file/insert_csv\n","    \"\"\"\n","\n","    # Start the Hyper process.\n","    #\n","    # * Sending telemetry data to Tableau is encouraged when trying out an experimental feature.\n","    #   To opt out, simply set `telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU` below.\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n","\n","        # Open a connection to the Hyper process. This will also create the new Hyper file.\n","        # The `CREATE_AND_REPLACE` mode causes the file to be replaced if it\n","        # already exists.\n","        with Connection(endpoint=hyper.endpoint,\n","                        database=hyper_database_path,\n","                        create_mode=CreateMode.CREATE_AND_REPLACE) as connection:\n","\n","            # Create the target table.\n","            connection.catalog.create_table(table_definition=table_definition)\n","\n","            # Execute a COPY command to insert the data from the Parquet file.\n","            copy_command = f\"COPY {table_definition.table_name} FROM {escape_string_literal(str(parquet_file_path))} WITH (FORMAT PARQUET)\"\n","            print(copy_command)\n","            count_inserted = connection.execute_command(copy_command)\n","            print(f\"-- {count_inserted} rows have been copied from '{parquet_file_path}' to the table {table_definition.table_name} in '{hyper_database_path}'.\")\n","\n","\n","if __name__ == \"__main__\":\n","  try:\n","    pq_path    = Path(\"/content/drive/MyDrive/green_trip_data/green_tripdata_processed.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")    # → /…/green_tripdata_processed.hyper\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"green_tripdata_processed\"),\n","        columns=[\n","            TableDefinition.Column(\"pickup_date\",          SqlType.date(),                          NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",          SqlType.time(),                          NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",         SqlType.date(),                          NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",         SqlType.time(),                          NULLABLE),\n","            TableDefinition.Column(\"trip_time\",            SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"pickup_location_id\",   SqlType.big_int(),                       NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\",  SqlType.big_int(),                       NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",        SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"fare_amount\",          SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"extra\",                SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"mta_tax\",              SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"tip_amount\",           SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"tolls_amount\",         SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"improvement_surcharge\",SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"total_amount\",         SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\", SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",   SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",        SqlType.double(),                        NULLABLE),\n","            TableDefinition.Column(\"RatecodeID\",           SqlType.text(),                          NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(pq_path, table_definition, hyper_path)\n","\n","  except HyperException as ex:\n","      print(ex)\n","      exit(1)\n","\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmu-FyFqKpqd","executionInfo":{"status":"ok","timestamp":1746534108603,"user_tz":420,"elapsed":36562,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"2b2e251d-a0e6-456f-d7ea-545d563846bf"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["COPY \"green_tripdata_processed\" FROM '/content/drive/MyDrive/green_trip_data/green_tripdata_processed.parquet' WITH (FORMAT PARQUET)\n","-- 4937066 rows have been copied from '/content/drive/MyDrive/green_trip_data/green_tripdata_processed.parquet' to the table \"green_tripdata_processed\" in '/content/drive/MyDrive/green_trip_data/green_tripdata_processed.hyper'.\n"]}]},{"cell_type":"markdown","source":["# Yellow"],"metadata":{"id":"l10mmQZDjVSX"}},{"cell_type":"code","source":["import dask.dataframe as dd\n","ddf = dd.read_parquet(\"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\")\n","print(ddf.head())  # Ensure readable data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"fKDVecabuHba","executionInfo":{"status":"ok","timestamp":1746537079109,"user_tz":420,"elapsed":14365,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"30389f9c-387a-448b-b515-44eb3193ffb4"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["  pickup_date pickup_time dropoff_date dropoff_time  trip_time  \\\n","0  2020-01-01    00:28:15   2020-01-01     00:33:03      288.0   \n","1  2020-01-01    00:35:39   2020-01-01     00:43:04      445.0   \n","2  2020-01-01    00:47:41   2020-01-01     00:53:52      371.0   \n","3  2020-01-01    00:55:23   2020-01-01     01:00:14      291.0   \n","5  2020-01-01    00:09:44   2020-01-01     00:10:37       53.0   \n","\n","   passenger_count  trip_distance  pickup_location_id  dropoff_location_id  \\\n","0              1.0           1.20                 238                  239   \n","1              1.0           1.20                 239                  238   \n","2              1.0           0.60                 238                  238   \n","3              1.0           0.80                 238                  151   \n","5              1.0           0.03                   7                  193   \n","\n","   fare_amount  ...  mta_tax  tip_amount  tolls_amount  improvement_surcharge  \\\n","0          6.0  ...      0.5        1.47           0.0                    0.3   \n","1          7.0  ...      0.5        1.50           0.0                    0.3   \n","2          6.0  ...      0.5        1.00           0.0                    0.3   \n","3          5.5  ...      0.5        1.36           0.0                    0.3   \n","5          2.5  ...      0.5        0.00           0.0                    0.3   \n","\n","   total_amount  congestion_surcharge  Airport_fee  cbd_congestion_fee  \\\n","0         13.77                   2.5          0.0                 0.0   \n","1         14.80                   2.5          0.0                 0.0   \n","2         13.30                   2.5          0.0                 0.0   \n","3          8.16                   0.0          0.0                 0.0   \n","5          3.80                   0.0          0.0                 0.0   \n","\n","   fare_per_mile     RatecodeID  \n","0      11.475000  Standard rate  \n","1      12.333333  Standard rate  \n","2      22.166667  Standard rate  \n","3      10.200000  Standard rate  \n","5     126.666667  Standard rate  \n","\n","[5 rows x 21 columns]\n"]}]},{"cell_type":"code","source":["import gc\n","del ddf\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmigKYWiuQgp","executionInfo":{"status":"ok","timestamp":1746537109456,"user_tz":420,"elapsed":29,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"274c5474-9f25-4e26-e4ba-4b07286b24cc"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["150"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["import pyarrow.parquet as pq\n","\n","# point this at whichever file you want to inspect\n","parquet_path = \"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\"\n","# create a ParquetFile object\n","pq_file = pq.ParquetFile(parquet_path)\n","\n","# print the Arrow schema (field names, types, nullability)\n","print(pq_file.schema_arrow)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"NDZed1gdX7Eg","executionInfo":{"status":"ok","timestamp":1746534139711,"user_tz":420,"elapsed":24,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"93c9c71d-d591-460c-fc44-83ed81888d84"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["pickup_date: string\n","pickup_time: string\n","dropoff_date: string\n","dropoff_time: string\n","trip_time: double\n","passenger_count: double\n","trip_distance: double\n","pickup_location_id: int64\n","dropoff_location_id: int64\n","fare_amount: double\n","extra: double\n","mta_tax: double\n","tip_amount: double\n","tolls_amount: double\n","improvement_surcharge: double\n","total_amount: double\n","congestion_surcharge: double\n","Airport_fee: double\n","cbd_congestion_fee: double\n","fare_per_mile: double\n","RatecodeID: string\n","index: int64\n","-- schema metadata --\n","pandas: '{\"index_columns\": [\"index\"], \"column_indexes\": [{\"name\": null, \"' + 2953\n"]}]},{"cell_type":"code","source":["def run_create_hyper_file_from_parquet(\n","        parquet_file_path: Path,\n","        table_definition: TableDefinition,\n","        hyper_database_path: Path):\n","\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n","\n","        with Connection(endpoint=hyper.endpoint,\n","                        database=hyper_database_path,\n","                        create_mode=CreateMode.CREATE_AND_REPLACE) as connection:\n","\n","            # Create the target table.\n","            connection.catalog.create_table(table_definition=table_definition)\n","\n","            # Execute a COPY command to insert the data from the Parquet file.\n","            copy_command = f\"COPY {table_definition.table_name} FROM {escape_string_literal(str(parquet_file_path))} WITH (FORMAT PARQUET)\"\n","            print(copy_command)\n","            count_inserted = connection.execute_command(copy_command)\n","            print(f\"-- {count_inserted} rows have been copied from '{parquet_file_path}' to the table {table_definition.table_name} in '{hyper_database_path}'.\")\n","\n","\n","if __name__ == \"__main__\":\n","  try:\n","    pq_path    = Path(\"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")    # → /…/green_tripdata_processed.hyper\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"combined_yellow\"),\n","        columns=[\n","\n","            TableDefinition.Column(\"pickup_date\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",          SqlType.text(),    NULLABLE),\n","\n","            TableDefinition.Column(\"trip_time\",             SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"passenger_count\",       SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",         SqlType.double(),  NULLABLE),\n","\n","\n","            TableDefinition.Column(\"pickup_location_id\",    SqlType.big_int(),  NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\",   SqlType.big_int(),  NULLABLE),\n","\n","            TableDefinition.Column(\"fare_amount\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"extra\",                 SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"mta_tax\",               SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tip_amount\",            SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tolls_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"improvement_surcharge\", SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"total_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\",  SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"Airport_fee\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",    SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",         SqlType.double(),  NULLABLE),\n","\n","            TableDefinition.Column(\"RatecodeID\",            SqlType.text(),    NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(pq_path, table_definition, hyper_path)\n","\n","  except HyperException as ex:\n","      print(ex)\n","      exit(1)\n","\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NbtOUhMi6D_","executionInfo":{"status":"ok","timestamp":1746536016065,"user_tz":420,"elapsed":1382616,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"9adb3810-0cc9-44cd-bb2b-789dc9c3c461"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["COPY \"combined_yellow\" FROM '/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet' WITH (FORMAT PARQUET)\n","current transaction is aborted, commands ignored until end of transaction block\n","Context: 0xfa6b0e2f\n"]}]},{"cell_type":"code","source":["from dask_cuda import LocalCUDACluster\n","from dask.distributed import Client\n","import dask_cudf\n","from pathlib import Path\n","from tableauhyperapi import (\n","    HyperProcess, Telemetry,\n","    Connection, CreateMode,\n","    SchemaName, TableName,\n","    NULLABLE, SqlType, TableDefinition,\n","    Inserter, HyperException\n",")\n","\n","# Cluster\n","cluster = LocalCUDACluster()\n","client  = Client(cluster)\n","\n","# Constants\n","INT_COLS = [\"pickup_location_id\", \"dropoff_location_id\"]\n","TEXT_COLS = [\"pickup_date\", \"pickup_time\", \"dropoff_date\", \"dropoff_time\"]\n","\n","START_PARTITION = 3\n","\n","def run_create_hyper_file_from_parquet(parquet_file_path: Path,\n","        table_definition: TableDefinition,\n","        hyper_database_path: Path,\n","        npartitions: int = None):\n","\n","    ddf = dask_cudf.read_parquet(str(parquet_file_path))\n","    if npartitions:\n","        ddf = ddf.repartition(npartitions=npartitions)\n","\n","    for c in INT_COLS:\n","        ddf[c] = ddf[c].astype('float64')\n","        mask   = ddf[c].isnull() | ((ddf[c] % 1) == 0)\n","        ddf    = ddf[mask]\n","        ddf[c]  = ddf[c].astype('int64')\n","\n","\n","    for c in TEXT_COLS:\n","        ddf[c] = ddf[c].astype('str')\n","\n","    ddf = ddf.persist()\n","\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper, \\\n","         Connection(endpoint=hyper.endpoint,\n","                    database=hyper_database_path,\n","                    create_mode=CreateMode.CREATE_AND_REPLACE if not hyper_database_path.exists() else CreateMode.NONE) as conn:\n","\n","        # conn.catalog.create_schema(schema=SchemaName(\"Extract\"))\n","        # conn.catalog.create_table(table_definition=table_definition)\n","        total_inserted = total_skipped = 0\n","\n","        for i, part in enumerate(ddf.to_delayed()):\n","            if i != START_PARTITION:\n","                print(f\"Skipping partition {i} (already processed)\")\n","                continue\n","\n","            try:\n","                gdf = part.compute()\n","                pdf = gdf.to_pandas()\n","                rows = list(pdf.itertuples(index=False, name=None))\n","                with Inserter(conn, table_definition) as inserter:\n","                    inserter.add_rows(rows)\n","                    inserter.execute()\n","                    total_inserted += len(rows)\n","                    print(f\"Inserted partition {i} ({len(rows)} rows)\")\n","            except HyperException as e:\n","                print(f\"Skipped partition {i} due to HyperException: {e}\")\n","                total_skipped += 1\n","\n","        print(f\"\\n✅ Finished. Inserted: {total_inserted}, Skipped: {total_skipped}\")\n","\n","\n","if __name__ == \"__main__\":\n","    pq_path    = Path(\"/content/drive/MyDrive/yellow_trip_data/combined_yellow.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"Extract\", \"combined_yellow\"),\n","        columns=[\n","            TableDefinition.Column(\"pickup_date\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"trip_time\",             SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"passenger_count\",       SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"pickup_location_id\",    SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\",   SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"fare_amount\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"extra\",                 SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"mta_tax\",               SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tip_amount\",            SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tolls_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"improvement_surcharge\", SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"total_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\",  SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"Airport_fee\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",    SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"RatecodeID\",            SqlType.text(),    NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(\n","        pq_path,\n","        table_definition,\n","        hyper_path,\n","        npartitions=8\n","    )\n"],"metadata":{"id":"zysW5T--v2f4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FHVHV"],"metadata":{"id":"0rNKhuUUjXeO"}},{"cell_type":"code","source":["import pyarrow.parquet as pq\n","\n","# point this at whichever file you want to inspect\n","parquet_path = \"/content/drive/MyDrive/fhvhv_trip_data/combined_fhvhv.parquet\"\n","\n","# create a ParquetFile object\n","pq_file = pq.ParquetFile(parquet_path)\n","\n","# print the Arrow schema (field names, types, nullability)\n","print(pq_file.schema)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2hrJTRTi6Wh","executionInfo":{"status":"ok","timestamp":1746541911802,"user_tz":420,"elapsed":240,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"a7cc01a6-22f9-428b-83b5-447d93a01da4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["<pyarrow._parquet.ParquetSchema object at 0x7cbbc3dcfb00>\n","required group field_id=-1 schema {\n","  optional int64 field_id=-1 pickup_location_id;\n","  optional int64 field_id=-1 dropoff_location_id;\n","  optional double field_id=-1 trip_distance;\n","  optional int64 field_id=-1 trip_time;\n","  optional double field_id=-1 base_passenger_fare;\n","  optional double field_id=-1 tolls;\n","  optional double field_id=-1 bcf;\n","  optional double field_id=-1 sales_tax;\n","  optional double field_id=-1 congestion_surcharge;\n","  optional double field_id=-1 airport_fee;\n","  optional double field_id=-1 tips;\n","  optional double field_id=-1 cbd_congestion_fee;\n","  optional binary field_id=-1 company_name (String);\n","  optional double field_id=-1 total_amount;\n","  optional double field_id=-1 fare_per_mile;\n","  optional binary field_id=-1 pickup_date (String);\n","  optional binary field_id=-1 pickup_time (String);\n","  optional binary field_id=-1 dropoff_date (String);\n","  optional binary field_id=-1 dropoff_time (String);\n","  optional int64 field_id=-1 index;\n","}\n","\n"]}]},{"cell_type":"code","source":["def run_create_hyper_file_from_parquet(\n","        parquet_file_path: Path,\n","        table_definition: TableDefinition,\n","        hyper_database_path: Path):\n","\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n","\n","        with Connection(endpoint=hyper.endpoint,\n","                        database=hyper_database_path,\n","                        create_mode=CreateMode.CREATE_AND_REPLACE) as connection:\n","\n","            # Create the target table.\n","            connection.catalog.create_table(table_definition=table_definition)\n","\n","            # Execute a COPY command to insert the data from the Parquet file.\n","            copy_command = f\"COPY {table_definition.table_name} FROM {escape_string_literal(str(parquet_file_path))} WITH (FORMAT PARQUET)\"\n","            print(copy_command)\n","            count_inserted = connection.execute_command(copy_command)\n","            print(f\"-- {count_inserted} rows have been copied from '{parquet_file_path}' to the table {table_definition.table_name} in '{hyper_database_path}'.\")\n","\n","\n","if __name__ == \"__main__\":\n","  try:\n","    pq_path    = Path(\"/content/drive/MyDrive/fhvhv_trip_data/combined_fhvhv.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")    # → /…/green_tripdata_processed.hyper\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"combined_fhvhv\"),\n","        columns=[\n","\n","            TableDefinition.Column(\"pickup_date\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",           SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",          SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",          SqlType.text(),    NULLABLE),\n","\n","            TableDefinition.Column(\"trip_time\",             SqlType.big_int(),  NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",         SqlType.double(),  NULLABLE),\n","\n","\n","            TableDefinition.Column(\"pickup_location_id\",    SqlType.big_int(),  NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\",   SqlType.big_int(),  NULLABLE),\n","\n","            TableDefinition.Column(\"base_passenger_fare\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"bcf\",                 SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"sales_tax\",               SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tips\",            SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tolls\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"total_amount\",          SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\",  SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"airport_fee\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",    SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"company_name\",            SqlType.text(),    NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(pq_path, table_definition, hyper_path)\n","\n","  except HyperException as ex:\n","      print(ex)\n","      exit(1)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dHKhq-VhjTrh","executionInfo":{"status":"ok","timestamp":1746543961037,"user_tz":420,"elapsed":1450032,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"217a2ade-8a16-4b7a-dea5-65e2ffb57703"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["COPY \"combined_fhvhv\" FROM '/content/drive/MyDrive/fhvhv_trip_data/combined_fhvhv.parquet' WITH (FORMAT PARQUET)\n","canceled\n","Context: 0xfa6b0e2f\n"]}]},{"cell_type":"code","source":["from dask_cuda import LocalCUDACluster\n","from dask.distributed import Client\n","import dask_cudf\n","import pandas as pd\n","from pathlib import Path\n","from tableauhyperapi import (\n","    HyperProcess, Telemetry,\n","    Connection, CreateMode,\n","    SchemaName, TableName,\n","    NULLABLE, SqlType, TableDefinition,\n","    Inserter, HyperException\n",")\n","\n","# start one GPU worker per card\n","cluster = LocalCUDACluster()\n","client  = Client(cluster)\n","\n","# your int/text columns\n","INT_COLS  = [\"trip_time\", \"pickup_location_id\", \"dropoff_location_id\"]\n","TEXT_COLS = [\"pickup_date\", \"pickup_time\", \"dropoff_date\", \"dropoff_time\"]\n","\n","def run_create_hyper_file_from_parquet(\n","    parquet_file_path: Path,\n","    table_definition: TableDefinition,\n","    hyper_database_path: Path,\n","    npartitions: int = None\n","):\n","    # 1) GPU-backed read\n","    ddf = dask_cudf.read_parquet(str(parquet_file_path))\n","    if npartitions:\n","        ddf = ddf.repartition(npartitions=npartitions)\n","\n","    # 2) GPU-side casts & filters\n","    for c in INT_COLS:\n","        # cast to float64 so non-whole get filtered\n","        ddf[c] = ddf[c].astype(\"float64\")\n","        mask   = ddf[c].isnull() | ((ddf[c] % 1) == 0)\n","        ddf     = ddf[mask]\n","        ddf[c] = ddf[c].astype(\"int64\")\n","\n","    for c in TEXT_COLS:\n","        ddf[c] = ddf[c].astype(\"str\")\n","\n","    # ←— Persist once, so following compute() is cached\n","    ddf = ddf.persist()\n","\n","    # 3) Hyper load\n","    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper, \\\n","         Connection(endpoint=hyper.endpoint,\n","                    database=hyper_database_path,\n","                    create_mode=CreateMode.CREATE_AND_REPLACE) as conn:\n","\n","        conn.catalog.create_schema(schema=SchemaName(\"Extract\"))\n","        conn.catalog.create_table(table_definition=table_definition)\n","\n","        total_inserted = total_skipped = 0\n","\n","        # 4) Stream each partition back to CPU\n","        for part in ddf.to_delayed():\n","            # gdf is a GPU DataFrame\n","            gdf = part.compute()\n","\n","            # convert to pandas (this moves data over PCIe)\n","            pdf = gdf.to_pandas()\n","\n","            # — ensure Python ints for Hyper\n","            for c in INT_COLS:\n","                # pdf[c] is pd.Series dtype int64\n","                # convert to object dtype with Python int or None\n","                pdf[c] = pdf[c].where(pdf[c].isna(), pdf[c].astype(int))\n","                pdf[c] = pdf[c].where(pdf[c].isna(), pdf[c])  # now dtype=object\n","\n","\n","            rows = list(pdf.itertuples(index=False, name=None))\n","\n","            # 6) Insert\n","            with Inserter(conn, table_definition) as inserter:\n","                try:\n","                    inserter.add_rows(rows)\n","                    inserter.execute()\n","                    total_inserted += len(rows)\n","                    print(f\"Inserted {len(rows)} rows (total so far: {total_inserted})\")\n","\n","                except HyperException as e:\n","                    total_skipped += len(rows)\n","                    print(f\"⚠️ Skipped partition due to error: {e}\")\n","                    conn.rollback()\n","\n","        print(f\"\\n✅ Finished. Inserted: {total_inserted}, Skipped: {total_skipped}\")\n","\n","\n","if __name__ == \"__main__\":\n","    pq_path    = Path(\"/content/drive/MyDrive/fhvhv_trip_data/combined_fhvhv.parquet\")\n","    hyper_path = pq_path.with_suffix(\".hyper\")\n","\n","    table_definition = TableDefinition(\n","        table_name=TableName(\"Extract\", \"combined_fhvhv\"),\n","        columns=[\n","            TableDefinition.Column(\"pickup_location_id\",  SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"dropoff_location_id\", SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"trip_distance\",       SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"trip_time\",           SqlType.big_int(), NULLABLE),\n","            TableDefinition.Column(\"base_passenger_fare\", SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tolls\",               SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"bcf\",                 SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"sales_tax\",           SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"congestion_surcharge\",SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"airport_fee\",         SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"tips\",                SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"cbd_congestion_fee\",  SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"company_name\",        SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"total_amount\",        SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"fare_per_mile\",       SqlType.double(),  NULLABLE),\n","            TableDefinition.Column(\"pickup_date\",         SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"pickup_time\",         SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_date\",        SqlType.text(),    NULLABLE),\n","            TableDefinition.Column(\"dropoff_time\",        SqlType.text(),    NULLABLE),\n","        ]\n","    )\n","\n","    run_create_hyper_file_from_parquet(\n","        pq_path,\n","        table_definition,\n","        hyper_path,\n","        npartitions=60  # adjust for ~600 MB chunks on 30 GB\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"b31hLOpPKFcb","executionInfo":{"status":"error","timestamp":1746555007262,"user_tz":420,"elapsed":4967994,"user":{"displayName":"Tin Phan","userId":"11573120625890018946"}},"outputId":"7c20b64b-9e42-4317-ec79-2c890a7a6dde"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n","INFO:distributed.scheduler:State start\n","INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-rp65wty3', purging\n","INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-er2h5rz4', purging\n","INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:37973\n","INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n","INFO:distributed.scheduler:Registering Worker plugin shuffle\n","INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38461'\n","INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:41173 name: 0\n","INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41173\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:59720\n","INFO:distributed.scheduler:Receive client connection: Client-c9e1a8c3-2a99-11f0-9324-0242ac1c0002\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:59736\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 29.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 27.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"]},{"output_type":"stream","name":"stdout","text":["Inserted 34603008 rows (total so far: 34603008)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:distributed.core:Event loop was unresponsive in Scheduler for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Connection to tcp://127.0.0.1:59720 has been closed.\n","INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:41173 name: 0 (stimulus_id='handle-worker-cleanup-1746554073.0521348')\n","WARNING:distributed.scheduler:Removing worker 'tcp://127.0.0.1:41173' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 14), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 17), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 23), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 20), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 26), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 32), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 29), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 35), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 38), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 44), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 41), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 47), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 50), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 56), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 53), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 59), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 4), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 1), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 7), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 13), ('getitem-fused-assign-e332431027ba72d62c8fb23bf5ac3ad0', 1), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 10), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 16), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 19), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 25), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 22), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 28), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 31), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 37), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 34), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 40), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 43), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 49), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 46), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 52), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 58), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 55), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 0), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 6), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 3), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 9), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 12), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 18), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 15), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 21), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 24), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 30), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 27), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 33), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 39), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 36), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 42), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 45), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 51), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 48), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 54), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 57), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 2), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 5), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 11), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 8)} (stimulus_id='handle-worker-cleanup-1746554073.0521348')\n","INFO:distributed.scheduler:Lost all workers\n","INFO:distributed.nanny:Worker process 21680 was killed by signal 9\n","WARNING:distributed.nanny:Restarting worker\n","INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:39193 name: 0\n","INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39193\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56998\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Scheduler for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Event loop was unresponsive in Nanny for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n","INFO:distributed.core:Connection to tcp://127.0.0.1:56998 has been closed.\n","INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:39193 name: 0 (stimulus_id='handle-worker-cleanup-1746554721.9601362')\n","WARNING:distributed.scheduler:Removing worker 'tcp://127.0.0.1:39193' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 23), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 20), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 26), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 32), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 35), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 38), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 44), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 41), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 47), ('repartitiontomore-05d2fe94123f705ff89781aae556bb1a', 19), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 50), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 56), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 53), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 59), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 25), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 22), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 31), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 37), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 34), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 40), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 43), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 49), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 46), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 52), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 58), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 55), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 6), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 9), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 21), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 24), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 30), ('read_parquet-operation-8066b6a7338a4a43f28f7880d553f8f3', 18), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 33), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 39), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 36), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 42), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 45), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 51), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 48), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 54), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 57), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 2), ('getitem-fused-assign-4dbbabeef13b2f53be912de3b7ce94aa', 8)} (stimulus_id='handle-worker-cleanup-1746554721.9601362')\n","INFO:distributed.scheduler:Lost all workers\n","INFO:distributed.nanny:Worker process 38354 was killed by signal 9\n","WARNING:distributed.nanny:Restarting worker\n","INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:37903 name: 0\n","INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37903\n","INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37620\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipped partition due to error: current transaction is aborted, commands ignored until end of transaction block\n","Context: 0xfa6b0e2f\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'Inserter' object has no attribute 'clear'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHyperException\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e75150ec468c>\u001b[0m in \u001b[0;36mrun_create_hyper_file_from_parquet\u001b[0;34m(parquet_file_path, table_definition, hyper_database_path, npartitions)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0minserter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0minserter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tableauhyperapi/inserter.py\u001b[0m in \u001b[0;36madd_rows\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tableauhyperapi/inserter.py\u001b[0m in \u001b[0;36madd_row\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insert_value_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tableauhyperapi/inserter.py\u001b[0m in \u001b[0;36m__write_text\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0mError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyper_add_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi_from_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tableauhyperapi/impl/dllutil.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0merrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHyperException\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\nContext: 0xfa6b0e2f","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e75150ec468c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     run_create_hyper_file_from_parquet(\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mpq_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtable_definition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-e75150ec468c>\u001b[0m in \u001b[0;36mrun_create_hyper_file_from_parquet\u001b[0;34m(parquet_file_path, table_definition, hyper_database_path, npartitions)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mtotal_skipped\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"⚠️ Skipped partition due to error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0minserter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inserted {len(rows)} rows (total so far: {total_inserted})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Inserter' object has no attribute 'clear'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-h6cRtd0Ohp6"},"execution_count":null,"outputs":[]}]}